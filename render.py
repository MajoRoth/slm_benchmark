import json
import os
from pathlib import Path

from jinja2 import FileSystemLoader, Environment


def render_page(name, description, sections):
    loader = FileSystemLoader(searchpath="./templates")
    env = Environment(loader=loader)
    template = env.get_template("demo.html.jinja2")

    html = template.render(
        page_title="Hebrew TTS",
        description=description,
        sections=sections,

    )

    dir_name = Path(__file__).parent / name

    if not os.path.exists(dir_name):
        os.makedirs(dir_name)

    with open(dir_name / "index.html", 'w') as f:
        f.write(html)


def render_background():
    description = """
        background noises samples from FSD50K. we sampled recordings longer then 5 seconds and with only 1 leaf label.
    """

    dirs = list(Path("/Users/amitroth/PycharmProjects/slm_benchmark/audio/background_noises_samples").glob("*"))


    wavs = [
        [Path("../") / Path(os.path.relpath(p, os.path.dirname(__file__))) for p in dir.glob("*")] for dir in dirs
    ]

    background_sections = [
        {
            "title": f"FSD50K dataset samples",
            "description": f"sampled between all of the leaves in the ontology",
            "table": {
                "headers": ["class", "sample 1", "sample 2"],
                "content": [
                    [
                        f"{l[0].parts[-2]}",
                        f'"{str(l[0])}"',
                        f'"{str(l[1])}"' if len(l) >= 2 else "",

                    ] for l in wavs
                ]
            }
        }
    ]

    print(background_sections)

    render_page(name='background', description=description, sections=background_sections)


def render_emotions():
    description_emotion = """

        Emotional Speakers Compression for SLM Acoustic Benchmark. We generate the samples using a voice cloning model (XTTS) by giving prompts
                            from emotional dataset (RAVDESS).

                            speakers are between 1-24 (odd are men, even are women). Emotions are from ['happy', 'sad', 'neutral'], texts generated by chat GPT.
                            Each audio sample is chosen between 5 samples, when choosing the sample which minimizes word error rate with the text prompt.


        """


    with open("audio/sentiment_alignment/top_wer/metadata.json", 'r') as f:
        sentiment_json = json.load(f)

    emotion_sections = [
        {
            "title": f"sample {sample['sample']} with {text} text.",
            "description": f"{sample[f'{text}_text']}",
            "table": {
                "headers": ["speaker id", "sad sample", "happy sample"],
                "content": [
                    [
                        f"{speaker['speaker']}",
                        speaker['generated_audio'][f'sad_{text}'],
                        speaker['generated_audio'][f'happy_{text}'],

                    ] for speaker in sample['speakers']
                ]
            }
        } for sample in sentiment_json for text in ['happy', 'sad']
    ]

    render_page(name='emotions', description=description_emotion, sections=emotion_sections)



if __name__ == '__main__':
    render_background()
    render_emotions()